# NYC Taxi Data Pipeline & ML Prediction

Ce projet implante une architecture Big Data compl√®te pour le traitement, le stockage et l'analyse pr√©dictive des donn√©es de taxis new-yorkais. Il couvre l'ensemble du cycle de vie de la donn√©e : de la collecte brute au dashboard de visualisation, en passant par un Data Warehouse et un mod√®le de Machine Learning performant.

## üöÄ Architecture du Projet

* **Data Lake (MinIO)** : Stockage des donn√©es brutes (Bronze) et nettoy√©es (Silver).
* **ETL (Apache Spark / Scala)** : Nettoyage, filtrage et ingestion massive.
* **Data Warehouse (PostgreSQL)** : Mod√©lisation en √©toile (Star Schema) pour l'analyse.
* **Machine Learning (Scikit-Learn)** : Pr√©diction du montant total des courses.
* **Visualisation (Streamlit)** : D√©monstration de l'utilisation du mod√®le

---

## Pr√©requis
* Java 11 Eclipse Temurin
* sbt via sdkman par exemple
* uv t√©l√©chargeable avec la commande ```curl -LsSf https://astral.sh/uv/install.sh | sh```
* Docker

---

## üõ†Ô∏è Guide d'installation et de lancement

### 1. Infrastructure (Docker)

Lancez les services MinIO et PostgreSQL :

```bash
docker-compose up -d --build
# V√©rifiez que les containers sont "Up"
docker ps
```
Si la commande ```docker-compose``` ne fonctionne pas utiliser ```docker compose``` sans le -.

Acc√©dez √† l'interface MinIO [http://localhost:9001](http://localhost:9001/login) avec, pour notre exemple, comme identifiant "minio" pour mot de passe "minio123" et cr√©ez manuellement les buckets :
- "nyc-raw"
- "nyc-cleaned"

### 2. Collecte et Ingestion (Scala/Spark)
#### √âtape 1 : R√©cup√©ration des donn√©es brutes

```bash
cd ex01_data_retrieval
sbt run
```

#### √âtape 2 : Nettoyage et Ingestion (DWH)

```bash
cd ../ex02_data_ingestion
sbt run
```

V√©rification du volume :
```bash
docker exec -it postgres-db psql -U myuser -d taxidb -c "SELECT count(*) FROM dwh.fact_trip;"
```

### 3. Machine Learning & Qualit√© (Python)
#### Pr√©parez l'environnement et entra√Ænez le mod√®le :

```bash
cd ../ex05_ml_prediction_service
uv venv .venv --python 3.11
source .venv/bin/activate
uv pip install -r requirements.txt
```

#### Configuration des acc√®s MinIO
```bash
export MINIO_ENDPOINT="http://localhost:9000"
export MINIO_ACCESS_KEY="minio"
export MINIO_SECRET_KEY="minio123"
```

#### Entra√Ænement
```bash
python -m src.train

```
#### V√©rifications Qualit√© :
Linting (PEP 8)
```bash
flake8 --max-line-length=100 src/
```

#### Tests Unitaires
```bash
export MINIO_BUCKET_CLEAN="nyc-cleaned"
export YEAR="2024"
export MONTH="01"
python -m pytest tests/test_pipeline.py
```

### 4. D√©monstration

Lancez l'application Streamlit :

```bash
streamlit run streamlit_app/app.py
```

---

## üìà R√©sultats et Performances

* Volume de donn√©es : Ingestion r√©ussie de plusieurs millions de lignes dans le Data Warehouse PostgreSQL.

* Pr√©cision du Mod√®le : RMSE de 4.25 obtenu sur la pr√©diction du total_amount (Cible : < 10).

* Mod√®le utilis√© : HistGradientBoostingRegressor (robuste aux grands volumes tabulaires).

* Qualit√© : Documentation au format NumpyDoc et conformit√© PEP 8.

---

## üßπ Nettoyage
Pour arr√™ter les services et supprimer les donn√©es persistantes (volumes) :

```bash
deactivate
rm -rf .venv/
cd ..
docker-compose down -v
```
